{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91668020-2c5b-4efd-963f-f6de69b995dd",
   "metadata": {},
   "source": [
    "# Load and Isolate the Nodule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8f753cce-b8ca-42a4-9f69-b315e3d4be62",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 3 patient folders in ../test.\n",
      "\n",
      "Processing patient 3304...\n",
      "\n",
      "Processing patient 3304 from folder: ../test/3304\n",
      "Saved CT scan (HU) for patient 3304 at: ../test_processed/3304/3304_full_volume.npy\n",
      "CT volume loaded for patient 3304 with shape: (305, 512, 512)\n",
      "\n",
      "Processing nodule index 24 for patient 3304\n",
      "Displaying extracted nodule cube for nodule index 24.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "display_middle_slice() missing 1 required positional argument: 'ground_truth'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[32], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m cube \u001b[38;5;241m=\u001b[39m load_and_save\u001b[38;5;241m.\u001b[39mextract_cube(ct_volume, center, cube_size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m, \u001b[38;5;241m128\u001b[39m))\n\u001b[1;32m     59\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDisplaying extracted nodule cube for nodule index \u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 60\u001b[0m \u001b[43mload_and_save\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdisplay_middle_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcube\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpatient_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpredicted_class\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mN/A\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# 3. 2D Magic Wand Segmentation\u001b[39;00m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;66;03m# -------------------------------\u001b[39;00m\n\u001b[1;32m     65\u001b[0m slice_img \u001b[38;5;241m=\u001b[39m ct_volume[seed_slice]\n",
      "\u001b[0;31mTypeError\u001b[0m: display_middle_slice() missing 1 required positional argument: 'ground_truth'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import our custom modules\n",
    "from modules.test_modules import load_and_save, cmw_asrg\n",
    "\n",
    "# -------------------------------\n",
    "# Base directories and CSV path\n",
    "# -------------------------------\n",
    "test_dir = os.path.join(\"..\", \"test\")             # Contains patient DICOM folders\n",
    "output_base = os.path.join(\"..\", \"test_processed\")  # Where processed volumes and masks will be saved\n",
    "csv_path = os.path.join(\"..\", \"test\", \"CombinedCancerList.csv\")\n",
    "\n",
    "# Load the global CSV with nodule coordinates.\n",
    "df_coords = pd.read_csv(csv_path)\n",
    "\n",
    "# Get list of all patient directories in the test folder.\n",
    "patient_ids = [d for d in os.listdir(test_dir) if os.path.isdir(os.path.join(test_dir, d))]\n",
    "print(f\"Found {len(patient_ids)} patient folders in {test_dir}.\")\n",
    "\n",
    "# Loop over each patient folder.\n",
    "for patient_id in patient_ids:\n",
    "    print(f\"\\nProcessing patient {patient_id}...\")\n",
    "    \n",
    "    # Define input and output directories.\n",
    "    dicom_dir = os.path.join(test_dir, patient_id)\n",
    "    output_dir = os.path.join(output_base, patient_id)\n",
    "    \n",
    "    # Process the CT scan: load DICOM series, convert to HU, and save volume.\n",
    "    load_and_save.process_patient_scan(dicom_dir, patient_id, output_dir)\n",
    "    \n",
    "    # Load the processed CT volume.\n",
    "    npy_filename = f\"{patient_id}_full_volume.npy\"\n",
    "    npy_path = os.path.join(output_dir, npy_filename)\n",
    "    if not os.path.isfile(npy_path):\n",
    "        print(f\"Processed CT volume for patient {patient_id} not found, skipping.\")\n",
    "        continue\n",
    "    ct_volume = np.load(npy_path)\n",
    "    print(f\"CT volume loaded for patient {patient_id} with shape: {ct_volume.shape}\")\n",
    "    \n",
    "    # Filter the CSV for nodules corresponding to this patient.\n",
    "    df_patient = df_coords[df_coords[\"uuid\"].astype(str) == patient_id]\n",
    "    if df_patient.empty:\n",
    "        print(f\"No nodule data found for patient {patient_id} in {csv_path}.\")\n",
    "        continue\n",
    "    \n",
    "    # Process each nodule for this patient.\n",
    "    for idx, row in df_patient.iterrows():\n",
    "        print(f\"\\nProcessing nodule index {idx} for patient {patient_id}\")\n",
    "        seed_slice = int(row[\"slice\"])\n",
    "        x_center = int(row[\"x\"])\n",
    "        y_center = int(row[\"y\"])\n",
    "        # Define the center as (z, y, x)\n",
    "        center = (seed_slice, y_center, x_center)\n",
    "        \n",
    "        # Extract a 128x128x128 cube around the cancer location.\n",
    "        cube = load_and_save.extract_cube(ct_volume, center, cube_size=(128, 128, 128))\n",
    "        print(f\"Displaying extracted nodule cube for nodule index {idx}.\")\n",
    "        load_and_save.display_middle_slice(cube, patient_id, idx, predicted_class=\"N/A\")\n",
    "        \n",
    "        # -------------------------------\n",
    "        # 3. 2D Magic Wand Segmentation\n",
    "        # -------------------------------\n",
    "        slice_img = ct_volume[seed_slice]\n",
    "        # Segmentation parameters (example values)\n",
    "        min_radius = 15\n",
    "        max_radius = 20\n",
    "        roughness = 2\n",
    "        zoom_factor = 1\n",
    "        center_range = 2\n",
    "        \n",
    "        mask_2d = cmw_asrg.cell_magic_wand(slice_img, (y_center, x_center),\n",
    "                                            min_radius, max_radius,\n",
    "                                            roughness=roughness, zoom_factor=zoom_factor,\n",
    "                                            center_range=center_range)\n",
    "        print(f\"Displaying 2D Magic Wand mask for nodule index {idx} on slice {seed_slice}.\")\n",
    "        # Wrap the 2D mask as a 3D volume to display.\n",
    "        cmw_asrg.display_mask_slice(np.expand_dims(mask_2d, axis=0), 0)\n",
    "        \n",
    "        # -------------------------------\n",
    "        # 4. 3D Magic Wand Segmentation\n",
    "        # -------------------------------\n",
    "        slices_above_below = 5  # Number of slices to include above and below the seed.\n",
    "        cancer_mask_3d = cmw_asrg.create_3d_cancer_mask(ct_volume, seed_slice, (y_center, x_center),\n",
    "                                                         min_radius, max_radius,\n",
    "                                                         roughness, zoom_factor, center_range, slices_above_below)\n",
    "        print(f\"Displaying 3D Magic Wand mask for nodule index {idx} on seed slice {seed_slice}.\")\n",
    "        cmw_asrg.display_mask_slice(cancer_mask_3d, seed_slice)\n",
    "        \n",
    "        # -------------------------------\n",
    "        # 5. Automated Seeded Region Growing (ASRG)\n",
    "        # -------------------------------\n",
    "        try:\n",
    "            seed_asrg = cmw_asrg.get_seed_point(cancer_mask_3d)\n",
    "        except Exception as e:\n",
    "            print(f\"Error obtaining ASRG seed for nodule {idx}: {e}\")\n",
    "            continue\n",
    "        asrg_threshold = 200  # Intensity threshold (adjust as needed)\n",
    "        asrg_mask = cmw_asrg.region_growing(ct_volume, cancer_mask_3d, seed_asrg, threshold=asrg_threshold)\n",
    "        asrg_mask = (asrg_mask > 0).astype(np.uint8)\n",
    "        print(f\"Displaying ASRG mask for nodule index {idx} on slice {seed_asrg[0]}.\")\n",
    "        cmw_asrg.display_extracted_region(ct_volume, asrg_mask, seed_asrg[0])\n",
    "        \n",
    "        # -------------------------------\n",
    "        # 6. Save the ASRG Mask\n",
    "        # -------------------------------\n",
    "        asrg_filename = f\"{idx}_asrg_mask.npy\"\n",
    "        asrg_path = os.path.join(output_dir, asrg_filename)\n",
    "        np.save(asrg_path, asrg_mask)\n",
    "        print(f\"Saved ASRG mask for nodule index {idx} at: {asrg_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efa7e122-02f5-4156-af52-1a79fa1df66e",
   "metadata": {},
   "source": [
    "# Extract Features for Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4bc6d8-4cfe-436c-888e-bf1f052b15c0",
   "metadata": {},
   "source": [
    "## Shape Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cf32fa76-99ce-4f8c-91f9-1cc7530eaa01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape feature extraction complete. CSV saved at: ../test_processed/test_features.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import the shape-based feature extraction function.\n",
    "from modules.shape_features import extract_nodule_features\n",
    "\n",
    "# Base directory where processed CT scans and masks are saved.\n",
    "processed_root = \"../test_processed\"\n",
    "\n",
    "# List to store features from each nodule.\n",
    "all_features = []\n",
    "\n",
    "# Iterate over each patient directory directly under processed_root.\n",
    "patient_dirs = [\n",
    "    os.path.join(processed_root, d) for d in os.listdir(processed_root)\n",
    "    if os.path.isdir(os.path.join(processed_root, d))\n",
    "]\n",
    "\n",
    "for patient_dir in patient_dirs:\n",
    "    patient_id = os.path.basename(patient_dir)\n",
    "    # Look for nodule masks: try ASRG masks first; if none, try CMW masks.\n",
    "    mask_files = [f for f in os.listdir(patient_dir) if f.endswith(\"_asrg_mask.npy\")]\n",
    "    if not mask_files:\n",
    "        mask_files = [f for f in os.listdir(patient_dir) if f.endswith(\"_cmw_mask.npy\")]\n",
    "    if not mask_files:\n",
    "        print(f\"No nodule masks found for patient {patient_id} in {patient_dir}.\")\n",
    "        continue\n",
    "\n",
    "    # Process each nodule mask.\n",
    "    for mask_file in mask_files:\n",
    "        # Assume the mask filename is like \"1_asrg_mask.npy\" or \"1_cmw_mask.npy\"\n",
    "        nodule_number = mask_file.split(\"_\")[0]\n",
    "        mask_path = os.path.join(patient_dir, mask_file)\n",
    "        try:\n",
    "            mask = np.load(mask_path)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading mask {mask_file} for patient {patient_id}: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Extract shape-based features (adjust spacing if needed).\n",
    "        shape_feats = extract_nodule_features(mask, spacing=(1, 1, 1))\n",
    "        \n",
    "        # Build the feature dictionary including nodule number and patient id.\n",
    "        feature_dict = {\n",
    "            \"nodule_number\": nodule_number,\n",
    "            \"patient_id\": patient_id\n",
    "        }\n",
    "        feature_dict.update(shape_feats)\n",
    "        \n",
    "        # Remove keys we don't want to save.\n",
    "        for key in [\"wavelet_entropy_level_1\", \"intensity_std\"]:\n",
    "            if key in feature_dict:\n",
    "                feature_dict.pop(key)\n",
    "                \n",
    "        all_features.append(feature_dict)\n",
    "\n",
    "# Create a DataFrame from the collected features.\n",
    "df_shape = pd.DataFrame(all_features)\n",
    "\n",
    "# Save the features to a CSV file at the processed_root.\n",
    "output_csv_path = os.path.join(processed_root, \"test_features.csv\")\n",
    "df_shape.to_csv(output_csv_path, index=False)\n",
    "print(f\"Shape feature extraction complete. CSV saved at: {output_csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80f1d615-aae3-4102-b1de-76b6cd3ef1d4",
   "metadata": {},
   "source": [
    "## Texture Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9ae2a282-203e-45d5-82ba-f9229c85371d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Texture-based feature extraction complete. Updated CSV saved at: ../test_processed/test_features.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import the texture-based feature extraction function and the bounding box helper.\n",
    "from modules.texture_features import extract_texture_features\n",
    "from modules.artifact_noise_features import get_bounding_box\n",
    "\n",
    "# Base directory for processed data.\n",
    "data_processed_root = \"../test_processed\"\n",
    "\n",
    "# Path to the existing features CSV (from the shape-based step).\n",
    "csv_path = os.path.join(data_processed_root, \"test_features.csv\")\n",
    "\n",
    "# Load the existing features DataFrame.\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# List to store texture feature dictionaries.\n",
    "texture_feature_list = []\n",
    "\n",
    "# Iterate over each row in the CSV.\n",
    "for index, row in df.iterrows():\n",
    "    # Convert patient_id to integer (if possible) then to string to avoid decimal representation.\n",
    "    try:\n",
    "        patient_id = str(int(float(row['patient_id'])))\n",
    "    except Exception as e:\n",
    "        patient_id = str(row['patient_id'])\n",
    "    \n",
    "    # Similarly, ensure nodule_number is a clean string.\n",
    "    try:\n",
    "        nodule_number = str(int(float(row['nodule_number'])))\n",
    "    except Exception as e:\n",
    "        nodule_number = str(row['nodule_number'])\n",
    "    \n",
    "    # Use class information if available; otherwise, assume patient folder is directly under processed_root.\n",
    "    cls = str(row['class']) if 'class' in row else \"\"\n",
    "    \n",
    "    if cls:\n",
    "        patient_dir = os.path.join(data_processed_root, cls, patient_id)\n",
    "    else:\n",
    "        patient_dir = os.path.join(data_processed_root, patient_id)\n",
    "        \n",
    "    ct_scan_path = os.path.join(patient_dir, f\"{patient_id}_full_volume.npy\")\n",
    "    \n",
    "    if not os.path.isfile(ct_scan_path):\n",
    "        print(f\"CT scan not found for patient {patient_id}. Skipping row.\")\n",
    "        texture_feature_list.append({})\n",
    "        continue\n",
    "    ct_volume = np.load(ct_scan_path)\n",
    "    \n",
    "    # Look for the nodule mask: first try ASRG, then fall back to CMW.\n",
    "    mask_filename = f\"{nodule_number}_asrg_mask.npy\"\n",
    "    mask_path = os.path.join(patient_dir, mask_filename)\n",
    "    if not os.path.isfile(mask_path):\n",
    "        mask_filename = f\"{nodule_number}_cmw_mask.npy\"\n",
    "        mask_path = os.path.join(patient_dir, mask_filename)\n",
    "        if not os.path.isfile(mask_path):\n",
    "            print(f\"Mask not found for patient {patient_id}, nodule {nodule_number}. Skipping row.\")\n",
    "            texture_feature_list.append({})\n",
    "            continue\n",
    "    mask = np.load(mask_path)\n",
    "    \n",
    "    # Use the bounding box of the mask to extract the ROI from the CT scan.\n",
    "    bbox = get_bounding_box(mask)\n",
    "    if bbox is None:\n",
    "        print(f\"No nodule found in mask for patient {patient_id}, nodule {nodule_number}. Skipping row.\")\n",
    "        texture_feature_list.append({})\n",
    "        continue\n",
    "    roi = ct_volume[bbox]\n",
    "    \n",
    "    # Extract texture-based features from the ROI.\n",
    "    texture_feats = extract_texture_features(roi)\n",
    "    texture_feature_list.append(texture_feats)\n",
    "\n",
    "# Convert the texture feature dictionaries into a DataFrame.\n",
    "df_texture = pd.DataFrame(texture_feature_list)\n",
    "\n",
    "# Append the new texture columns to the original DataFrame.\n",
    "df_updated = pd.concat([df, df_texture], axis=1)\n",
    "\n",
    "# Save the updated DataFrame back to the existing CSV.\n",
    "df_updated.to_csv(csv_path, index=False)\n",
    "print(f\"Texture-based feature extraction complete. Updated CSV saved at: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26f7f5ea-8174-4b67-9109-ed0537fe8dfc",
   "metadata": {},
   "source": [
    "## Intensity Based Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2440998b-639a-4ef9-a37a-188b742a970a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intensity-based feature extraction complete. Updated CSV saved at: ../test_processed/test_features.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Import the intensity-based feature extraction function.\n",
    "from modules.statistic_features import extract_intensity_features\n",
    "# Reuse the bounding box helper from the artifact_noise_features module.\n",
    "from modules.artifact_noise_features import get_bounding_box\n",
    "\n",
    "# Base directory for processed data.\n",
    "data_processed_root = \"../test_processed\"\n",
    "\n",
    "# Path to the existing features CSV file (from the shape-based or previous steps).\n",
    "csv_path = os.path.join(data_processed_root, \"test_features.csv\")\n",
    "\n",
    "# Load the existing DataFrame.\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "# List to store intensity feature dictionaries.\n",
    "intensity_feature_list = []\n",
    "\n",
    "# Iterate over each row in the CSV.\n",
    "for index, row in df.iterrows():\n",
    "    # Convert patient_id and nodule_number to integer strings to avoid decimal representations.\n",
    "    try:\n",
    "        patient_id = str(int(float(row['patient_id'])))\n",
    "    except Exception as e:\n",
    "        patient_id = str(row['patient_id'])\n",
    "    try:\n",
    "        nodule_number = str(int(float(row['nodule_number'])))\n",
    "    except Exception as e:\n",
    "        nodule_number = str(row['nodule_number'])\n",
    "    \n",
    "    # Use class information if available; otherwise, assume the patient folder is directly under data_processed_root.\n",
    "    cls = str(row['class']) if 'class' in row else \"\"\n",
    "    if cls:\n",
    "        patient_dir = os.path.join(data_processed_root, cls, patient_id)\n",
    "    else:\n",
    "        patient_dir = os.path.join(data_processed_root, patient_id)\n",
    "    \n",
    "    # Build the CT scan path.\n",
    "    ct_scan_path = os.path.join(patient_dir, f\"{patient_id}_full_volume.npy\")\n",
    "    if not os.path.isfile(ct_scan_path):\n",
    "        print(f\"CT scan not found for patient {patient_id}. Skipping row.\")\n",
    "        intensity_feature_list.append({})\n",
    "        continue\n",
    "    ct_volume = np.load(ct_scan_path)\n",
    "    \n",
    "    # Look for the nodule mask: first try ASRG, then fall back to CMW.\n",
    "    mask_filename = f\"{nodule_number}_asrg_mask.npy\"\n",
    "    mask_path = os.path.join(patient_dir, mask_filename)\n",
    "    if not os.path.isfile(mask_path):\n",
    "        mask_filename = f\"{nodule_number}_cmw_mask.npy\"\n",
    "        mask_path = os.path.join(patient_dir, mask_filename)\n",
    "        if not os.path.isfile(mask_path):\n",
    "            print(f\"Mask not found for patient {patient_id}, nodule {nodule_number}. Skipping row.\")\n",
    "            intensity_feature_list.append({})\n",
    "            continue\n",
    "    mask = np.load(mask_path)\n",
    "    \n",
    "    # Use the bounding box of the mask to extract the ROI from the CT scan.\n",
    "    bbox = get_bounding_box(mask)\n",
    "    if bbox is None:\n",
    "        print(f\"No nodule found in mask for patient {patient_id}, nodule {nodule_number}. Skipping row.\")\n",
    "        intensity_feature_list.append({})\n",
    "        continue\n",
    "    roi = ct_volume[bbox]\n",
    "    \n",
    "    # Extract intensity-based features (statistical moments and histogram features).\n",
    "    intensity_feats = extract_intensity_features(roi, num_bins=50)\n",
    "    intensity_feature_list.append(intensity_feats)\n",
    "\n",
    "# Convert the list of intensity feature dictionaries into a DataFrame.\n",
    "df_intensity = pd.DataFrame(intensity_feature_list)\n",
    "\n",
    "# Append the new intensity feature columns to the original DataFrame.\n",
    "df_updated = pd.concat([df, df_intensity], axis=1)\n",
    "\n",
    "# Save the updated DataFrame back to the existing CSV.\n",
    "df_updated.to_csv(csv_path, index=False)\n",
    "print(f\"Intensity-based feature extraction complete. Updated CSV saved at: {csv_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19061689-7ed3-4378-997f-07d2b7a7a52a",
   "metadata": {},
   "source": [
    "# Perform Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d39fcdc-0f75-4594-99a4-8c9661738a2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from ../models/random_forest_model.pkl\n",
      "Loaded test features from ../test_processed/test_features.csv\n",
      "Predictions for test scan:\n",
      "   patient_id  nodule_number  predicted_class\n",
      "0        3304             24  FakeAddedCancer\n",
      "1        2190             61    RealCancerous\n",
      "2        2190             62    RealCancerous\n",
      "3        3246             22  FakeAddedCancer\n",
      "4        2131             57    RealCancerous\n",
      "5        2131             56    RealCancerous\n",
      "6        2131             58    RealCancerous\n",
      "Predictions saved to ../test_processed/test_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "# -------------------------------\n",
    "# Load the saved Random Forest model\n",
    "# -------------------------------\n",
    "model_path = os.path.join(\"..\", \"models\", \"random_forest_model.pkl\")\n",
    "model = joblib.load(model_path)\n",
    "print(f\"Loaded model from {model_path}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Load test features from CSV\n",
    "# -------------------------------\n",
    "test_features_csv = os.path.join(\"..\", \"test_processed\", \"test_features.csv\")\n",
    "df_test = pd.read_csv(test_features_csv)\n",
    "print(f\"Loaded test features from {test_features_csv}\")\n",
    "\n",
    "# -------------------------------\n",
    "# Prepare the features for prediction\n",
    "# -------------------------------\n",
    "# Remove non-feature columns (adjust if your CSV includes additional metadata)\n",
    "non_feature_cols = [\"nodule_number\", \"patient_id\"]\n",
    "X_test = df_test.drop(columns=non_feature_cols, errors=\"ignore\")\n",
    "\n",
    "# -------------------------------\n",
    "# Align test features with the model's expected features\n",
    "# -------------------------------\n",
    "# Use model.feature_names_in_ (available in scikit-learn 1.0+) to determine expected columns.\n",
    "if hasattr(model, \"feature_names_in_\"):\n",
    "    expected_features = model.feature_names_in_\n",
    "    # Add any missing columns with default value 0.0\n",
    "    for col in expected_features:\n",
    "        if col not in X_test.columns:\n",
    "            X_test[col] = 0.0\n",
    "    # Drop any extra columns that were not used during training.\n",
    "    X_test = X_test[expected_features]\n",
    "else:\n",
    "    print(\"Warning: The model does not have feature_names_in_ attribute. Ensure that the test features match the training features.\")\n",
    "\n",
    "# -------------------------------\n",
    "# Run predictions using the model\n",
    "# -------------------------------\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Map numeric predictions to class labels if needed.\n",
    "# Here we assume: 0 = RealCancerous and 1 = FakeAddedCancer.\n",
    "label_map = {0: \"RealCancerous\", 1: \"FakeAddedCancer\"}\n",
    "predicted_labels = [label_map.get(pred, pred) for pred in predictions]\n",
    "\n",
    "# Add predictions to the DataFrame.\n",
    "df_test[\"predicted_class\"] = predicted_labels\n",
    "\n",
    "# -------------------------------\n",
    "# Display and save the predictions\n",
    "# -------------------------------\n",
    "print(\"Predictions for test scan:\")\n",
    "print(df_test[[\"patient_id\", \"nodule_number\", \"predicted_class\"]])\n",
    "\n",
    "output_predictions_csv = os.path.join(\"..\", \"test_processed\", \"test_predictions.csv\")\n",
    "df_test.to_csv(output_predictions_csv, index=False)\n",
    "print(f\"Predictions saved to {output_predictions_csv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fd78e96-9766-4c1c-a201-e2934d135c39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lvenv",
   "language": "python",
   "name": "lvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
